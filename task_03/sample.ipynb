{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '.', 'It', \"'s\", 'amazing', 'to', 'process', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"NLP is fascinating. It's amazing to process language.\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '.', 'It', \"'s\", 'amazing', 'to', 'process', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"NLP is fascinating. It's amazing to process language.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'be', 'fascinating', '.', 'it', 'be', 'amazing', 'to', 'process', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def an_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "user_input = input(\"Enter a sentence: \")\n",
    "print(f\"Sentiment: {an_sentiment(user_input)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'everyone', '.', 'Good', 'morning', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "text=\"hello everyone. Good morning!!\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "result=word_tokenize(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello everyone.', 'Good morning!', '!']\n"
     ]
    }
   ],
   "source": [
    "#sentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    "result=sent_tokenize(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "ps.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'model', 'will', 'initially', 'have', 'no', 'way', 'of', 'knowing', 'that', 'dog', 'and', 'dogs', 'are', 'similar']\n"
     ]
    }
   ],
   "source": [
    "text=\"the model will initially have no way of knowing that dog and dogs are similar\"\n",
    "result=word_tokenize(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " ' ',\n",
       " 'model',\n",
       " ' ',\n",
       " 'will',\n",
       " ' ',\n",
       " 'initi',\n",
       " ' ',\n",
       " 'have',\n",
       " ' ',\n",
       " 'no',\n",
       " ' ',\n",
       " 'way',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'know',\n",
       " ' ',\n",
       " 'that',\n",
       " ' ',\n",
       " 'dog',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'dog',\n",
       " ' ',\n",
       " 'are',\n",
       " ' ',\n",
       " 'similar',\n",
       " ' ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=[]\n",
    "for i in result :\n",
    "    final.append(ps.stem(i))\n",
    "    final.append(\" \")\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def an_sentiment(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "  \n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0:\n",
    "        feedback = \"Positive sentiment 😊\"\n",
    "    elif sentiment < 0:\n",
    "        feedback = \"Negative sentiment 😞\"\n",
    "    else:\n",
    "        feedback = \"Neutral sentiment 😐\"\n",
    "\n",
    "    return {\n",
    "        \"Tokens\": tokens,\n",
    "        \"Lemmas\": lemmas,\n",
    "        \"Sentiment Feedback\": feedback\n",
    "    }\n",
    "\n",
    "user_input = input(\"Enter a sentence for analysis: \")\n",
    "results = an_sentiment(user_input)\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'we', 'want', 'to', 'completely', 'cover', 'a', 'language', 'with', 'a', 'word-based', 'tokenizer', ',', 'we', '’', 'll', 'need', 'to', 'have', 'an', 'identifier', 'for', 'each', 'word', 'in', 'the', 'language', ',', 'which', 'will', 'generate', 'a', 'huge', 'amount', 'of', 'tokens', '.', 'for', 'example', ',', 'there', 'are', 'over', '500,000', 'words', 'in', 'the', 'english', 'language', ',', 'so', 'to', 'build', 'a', 'map', 'from', 'each', 'word', 'to', 'an', 'input', 'id', 'we', '’', 'd', 'need', 'to', 'keep', 'track', 'of', 'that', 'many', 'ids', '.']\n"
     ]
    }
   ],
   "source": [
    "text=\"If we want to completely cover a language with a word-based tokenizer, we’ll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we’d need to keep track of that many IDs.\"\n",
    "text=text.lower()\n",
    "result=word_tokenize(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'we': 3,\n",
       " 'to': 5,\n",
       " 'a': 4,\n",
       " 'language': 3,\n",
       " ',': 4,\n",
       " '’': 2,\n",
       " 'need': 2,\n",
       " 'an': 2,\n",
       " 'for': 2,\n",
       " 'each': 2,\n",
       " 'word': 2,\n",
       " 'in': 2,\n",
       " 'the': 2,\n",
       " 'of': 2,\n",
       " '.': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict={}\n",
    "count=0\n",
    "for i in range(0,len(result)):\n",
    "    for j in range(0,len(result)):\n",
    "        if(result[i]==result[j]):\n",
    "            count=count+1\n",
    "    if(count>=2):\n",
    "        dict[result[i]]=count\n",
    "        count=0\n",
    "    else:\n",
    "        count=0\n",
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "final=[]\n",
    "flag=0\n",
    "for i in range(0,len(result)):\n",
    "    flag=0\n",
    "    for j in dict:\n",
    "        if(result[i]==j):\n",
    "            final.append(1)\n",
    "            flag=1\n",
    "            break\n",
    "    if(flag==0):\n",
    "        final.append(0)\n",
    "        \n",
    "print(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
